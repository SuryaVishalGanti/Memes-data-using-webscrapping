# -*- coding: utf-8 -*-
"""Meme Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Z13Uu9moPWGG4oxt66nzVrCLrHMycEW
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv

def scrape_gifs_to_csv(urls):
    rows = []

    base_url = "https://tenor.com"
    headers = {"User-Agent": "Mozilla/5.0"}

    for url in urls:
        print(f"\nScraping data from: {url}")

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")

            img_tags = soup.find_all("img")

            for img in img_tags:
                img_url = img.get("src")

                if img_url == "https://tenor.com/assets/img/tenor-logo.svg":
                    continue

                if img_url and not img_url.startswith("http"):
                    img_url = urljoin(base_url, img_url)

                img_name = img_url.split("/")[-1].split(".")[0]
                img_name_clean = img_name.replace("-", " ")

                rows.append([img_url, img_name_clean])
        else:
            print(f"Failed to retrieve webpage. Status code: {response.status_code}")

    with open("Telugu_memes.csv", mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)

        writer.writerow(["Image URL", "Text"])

        writer.writerows(rows)

    print("CSV file 'Telugu_memes.csv' has been created with the filtered data.")

urls = [
    "https://tenor.com/en-IN/search/sunil-memes-gifs",
    "https://tenor.com/en-IN/search/brahmanandam-memes-gifs",
    "https://tenor.com/en-IN/search/nuvvu-naaku-nachav-gifs",
    "https://tenor.com/en-IN/search/ready-movie-gifs",
    "https://tenor.com/en-IN/search/athadu-gifs",
    "https://tenor.com/search/telugu-memes-gifs",
    "https://tenor.com/search/telugu-funny-memes-gifs",
    "https://tenor.com/search/jabardasth-memes-gifs",
    "https://tenor.com/search/ali-memes-gifs",
    "https://tenor.com/search/ali-comedy-memes-gifs",
    "https://tenor.com/search/comedy-memes-telugu-gifs",
    "https://tenor.com/search/satya-comedy-gifs",
    "https://tenor.com/search/munna-bhai-mbbs-gifs",
    "https://tenor.com/search/vennela-kishore-gifs",
    "https://tenor.com/search/ms-narayana-gifs",
    "https://tenor.com/search/priyadarshi-gifs",
    "https://tenor.com/search/harsha-chemudu-gifs",
    "https://tenor.com/search/rahul-ramakrishna-gifs",
    "https://tenor.com/search/chiranjeevi-gifs",
    "https://tenor.com/search/balakrishna-gifs",
    "https://tenor.com/search/manmadhudu-gifs",
    "https://tenor.com/search/pawan-kalyan-gifs",
    "https://tenor.com/search/venkatesh-gifs",
    "https://tenor.com/search/ee-nagaraniki-emaindi-gifs",
    "https://tenor.com/search/f2-gifs",
    "https://tenor.com/search/anil-ravipudi-gifs",
    "https://tenor.com/search/dhee-gifs",
    "https://tenor.com/search/dubai-seenu-gifs",
]

scrape_gifs_to_csv(urls)





import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from openpyxl import Workbook

def scrape_gifs_to_excel(urls):
    wb = Workbook()
    ws = wb.active
    ws.title = "GIF Data"

    ws.append(["Image URL", "Text"])

    base_url = "https://tenor.com"
    headers = {"User-Agent": "Mozilla/5.0"}

    for url in urls:
        print(f"\nScraping data from: {url}")

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")

            img_tags = soup.find_all("img")

            for img in img_tags:
                img_url = img.get("src")

                if img_url == "https://tenor.com/assets/img/tenor-logo.svg":
                    continue

                if img_url and not img_url.startswith("http"):
                    img_url = urljoin(base_url, img_url)

                img_name = img_url.split("/")[-1].split(".")[0]
                img_name_clean = img_name.replace("-", " ")

                ws.append([img_url, img_name_clean])
        else:
            print(f"Failed to retrieve webpage. Status code: {response.status_code}")

    wb.save("Telugu_memes.xlsx")
    print("Excel file 'Telugu_memes.xlsx' has been created with the filtered data.")

urls = [
    "https://tenor.com/en-IN/search/sunil-memes-gifs",
    "https://tenor.com/en-IN/search/brahmanandam-memes-gifs",
    "https://tenor.com/en-IN/search/nuvvu-naaku-nachav-gifs",
    "https://tenor.com/en-IN/search/ready-movie-gifs",
    "https://tenor.com/en-IN/search/athadu-gifs",
    "https://tenor.com/search/telugu-memes-gifs",
    "https://tenor.com/search/telugu-funny-memes-gifs",
    "https://tenor.com/search/jabardasth-memes-gifs",
    "https://tenor.com/search/ali-memes-gifs",
    "https://tenor.com/search/ali-comedy-memes-gifs",
    "https://tenor.com/search/comedy-memes-telugu-gifs",
    "https://tenor.com/search/satya-comedy-gifs",
    "https://tenor.com/search/munna-bhai-mbbs-gifs",
    "https://tenor.com/search/vennela-kishore-gifs",
    "https://tenor.com/search/ms-narayana-gifs",
    "https://tenor.com/search/priyadarshi-gifs",
    "https://tenor.com/search/harsha-chemudu-gifs",
    "https://tenor.com/search/rahul-ramakrishna-gifs",
    "https://tenor.com/search/chiranjeevi-gifs",
    "https://tenor.com/search/balakrishna-gifs",
    "https://tenor.com/search/manmadhudu-gifs",
    "https://tenor.com/search/pawan-kalyan-gifs",
    "https://tenor.com/search/venkatesh-gifs",
    "https://tenor.com/search/ee-nagaraniki-emaindi-gifs",
    "https://tenor.com/search/f2-gifs",
    "https://tenor.com/search/anil-ravipudi-gifs",
    "https://tenor.com/search/dhee-gifs",
    "https://tenor.com/search/dubai-seenu-gifs",
]

scrape_gifs_to_excel(urls)



print("Total number of URLs:", len(urls))

